{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85dfbf00",
   "metadata": {},
   "source": [
    "# Aircraft Damage Classification and Multimodal Captioning with Cross-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189aa731",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision transformers tensorflow matplotlib scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae22e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models as keras_models\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7254a4d1",
   "metadata": {},
   "source": [
    "## Step 2: Load and Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47681486",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "data_dir = './sample_data'  # Replace with your dataset path\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=data_transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc0aac",
   "metadata": {},
   "source": [
    "## Step 3: Feature Extraction Using VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4345e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16(pretrained=True)\n",
    "for param in vgg16.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "feature_extractor = nn.Sequential(*list(vgg16.children())[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3630a9e",
   "metadata": {},
   "source": [
    "## Step 4: Build and Compile Keras VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7a937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "keras_model = keras_models.Sequential([\n",
    "    base_model,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(len(dataset.classes), activation='softmax')\n",
    "])\n",
    "\n",
    "keras_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcebcc75",
   "metadata": {},
   "source": [
    "## Step 5: Train the VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34b6482",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.random.rand(100, 224, 224, 3)\n",
    "train_labels = tf.keras.utils.to_categorical(np.random.randint(0, len(dataset.classes), 100), num_classes=len(dataset.classes))\n",
    "\n",
    "history = keras_model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    epochs=5,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389afa87",
   "metadata": {},
   "source": [
    "## Step 6: Plot Accuracy Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c536bde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80668956",
   "metadata": {},
   "source": [
    "## Step 7: Predict and Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ef205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = np.random.rand(9, 224, 224, 3)\n",
    "test_labels = np.random.randint(0, len(dataset.classes), 9)\n",
    "predictions = keras_model.predict(test_images)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "axes = axes.flatten()\n",
    "for img, ax, pred, true in zip(test_images, axes, predicted_classes, test_labels):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f'Pred: {pred}\\nTrue: {true}')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d252c58",
   "metadata": {},
   "source": [
    "## Step 8: Implement a Cross-Attention Layer in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc40d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.multi_head_attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embed_dim\n",
    "        )\n",
    "\n",
    "    def call(self, query, key, value):\n",
    "        attention_output = self.multi_head_attention(\n",
    "            query=query,\n",
    "            key=key,\n",
    "            value=value\n",
    "        )\n",
    "        return attention_output\n",
    "\n",
    "# Example usage of Cross-Attention\n",
    "image_features = tf.random.normal(shape=(2, 10, 512))\n",
    "text_features = tf.random.normal(shape=(2, 5, 512))\n",
    "\n",
    "cross_attention_layer = CrossAttention(embed_dim=512, num_heads=8)\n",
    "output = cross_attention_layer(query=image_features, key=text_features, value=text_features)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8622af",
   "metadata": {},
   "source": [
    "## Step 9: Generate Captions using BLIP Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d30bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "raw_image = Image.open('path_to_your_image.jpg').convert('RGB')\n",
    "inputs = processor(raw_image, return_tensors=\"pt\")\n",
    "out = blip_model.generate(**inputs)\n",
    "caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Generated Caption: {caption}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a33ff56",
   "metadata": {},
   "source": [
    "## Step 10: Show BLIP Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329f8eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(blip_model)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
